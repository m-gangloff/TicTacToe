{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from tic_env import TictactoeEnv, OptimalPlayer\n",
    "env = TictactoeEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Learning from experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empty_pos(grid):\n",
    "    '''return all empty positions of a grid'''\n",
    "    avail = []\n",
    "    for i in range(9):\n",
    "        pos = (int(i/3), i % 3)\n",
    "        if grid[pos] == 0:\n",
    "            avail.append(pos)\n",
    "    return avail\n",
    "\n",
    "def randomMove(grid):\n",
    "    \"\"\" Chose a random move from the available options. \"\"\"\n",
    "    avail = empty_pos(grid)\n",
    "    return avail[np.random.randint(0, len(avail)-1)]\n",
    "\n",
    "\n",
    "def find_Q_table_idx(Q_table, grid):\n",
    "    \"\"\"\n",
    "    Checks if the state is already in the Q table. If not adds it and then returns the index\n",
    "    \"\"\"\n",
    "    states = Q_table[:, 9]\n",
    "    #print(states.shape)\n",
    "    #print(states)\n",
    "    #idx = np.where(states == state)\n",
    "    #idx = np.where(np.array_equal(states, state))\n",
    "    #print(idx)\n",
    "    for idx, state in enumerate(states):\n",
    "        #print(type(state))\n",
    "        if np.array_equal(state, grid):\n",
    "            return idx\n",
    "        \n",
    "        # If the state is not an array, then no state that corresponds to the grid was found\n",
    "        elif isinstance(state, int):\n",
    "            Q_table[idx, 9] = grid\n",
    "            return idx\n",
    "\n",
    "    raise ValueError('There was no more space in the Q-Table to add a new entry')\n",
    "    \n",
    "\n",
    "def pick_action(state, Q_table, epsilon=0):\n",
    "    # implements greedy policy and returns the action with max. Q-value (given the state).\n",
    "    # note: when Q-table is filled with zeros, returns a random policy. \n",
    "    if np.random.random() < epsilon:\n",
    "        return randomMove(state)\n",
    "\n",
    "    # Get index of Q_table corresponding to current state\n",
    "    state_idx = find_Q_table_idx(Q_table, state)\n",
    "\n",
    "    # Get all the valid actions converted to integers between 0 and 8\n",
    "    valid_actions = [action_to_int(action) for action in empty_pos(state)]\n",
    "\n",
    "    # Create a tuple with the corresponding actions and Q values\n",
    "    action_scores = (valid_actions, Q_table[state_idx, :-1][valid_actions])\n",
    "\n",
    "    # Find max Q value\n",
    "    idxs_max_Q = np.where(action_scores[1] == np.max(action_scores[1]))[0]\n",
    "\n",
    "    # Return random chosen action corresponding to max Q value\n",
    "    return action_scores[0][np.random.choice(idxs_max_Q)]\n",
    "\n",
    "def action_to_int(tuple):\n",
    "    \"\"\"\n",
    "    Grids are of size 3x3. Hence to convert them to integers between 0 and 8, \n",
    "    we multiply the row idx by 3 and add the col idx\n",
    "    \"\"\"\n",
    "    return 3*tuple[0] + tuple[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(eps):\n",
    "    players = ['X', 'O']\n",
    "    Q_table = np.zeros((3**9, 10), dtype=object)\n",
    "    #print(Q_table.shape)\n",
    "    t_start = time.time()\n",
    "    game_start = 0\n",
    "    for game in range(20000):\n",
    "        # Change starting player after every game\n",
    "        players.reverse()\n",
    "        env.reset()\n",
    "        player_opt = OptimalPlayer(epsilon=0.5, player=players[0])\n",
    "        end = False\n",
    "        state, _, __ = env.observe()\n",
    "\n",
    "        # for step in range(9):\n",
    "        #     grid, _, __ = env.observe()\n",
    "        #     if env.current_player == player_opt.player:\n",
    "        #         move = player_opt.act(grid)\n",
    "        #     else:\n",
    "        #         move = pick_action(grid, Q_table, 0)\n",
    "                \n",
    "        #     grid_next, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "        #     state_idx = find_Q_table_idx(Q_table, grid)\n",
    "        #     state_next_idx = find_Q_table_idx(Q_table, grid_next)\n",
    "        #     reward = env.reward(players[1])\n",
    "        #     Q_table[state_idx, move] += eta * (reward + gamma * Q_table[state_next_idx, next_action] - Q_table[state_ind, action])\n",
    "\n",
    "        #     if end:\n",
    "        #         print('-------------------------------------------')\n",
    "        #         print('Game end, winner is player ' + str(winner))\n",
    "        #         print('Optimal player = ' +  players[0])\n",
    "        #         print('Agent player = ' +  players[1])\n",
    "        #         env.render()\n",
    "        #         env.reset()\n",
    "        #         break\n",
    "\n",
    "        if env.current_player == player_opt.player:\n",
    "            action = action_to_int(player_opt.act(state))\n",
    "        else:\n",
    "            action = pick_action(state, Q_table, eps)\n",
    "    \n",
    "        while not end:\n",
    "            state_idx = find_Q_table_idx(Q_table, state)\n",
    "\n",
    "            next_state, end, winner = env.step(action)\n",
    "            state_next_idx = find_Q_table_idx(Q_table, next_state)\n",
    "\n",
    "            if not end:\n",
    "                if env.current_player == player_opt.player:\n",
    "                    next_action = action_to_int(player_opt.act(next_state))\n",
    "                else:\n",
    "                    next_action = pick_action(next_state, Q_table, 0)\n",
    "\n",
    "            # update Q-table using the iterative update rule   \n",
    "            reward = env.reward(players[1])      \n",
    "            Q_table[state_idx, action] += 0.5 * (reward + 0.99 * Q_table[state_next_idx, next_action] - Q_table[state_idx, action])\n",
    "\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "        \n",
    "        if(game>19980 or game < 4):\n",
    "            print('-------------------------------------------')\n",
    "            print('Game {} ended, winner is player {}'.format(game, str(winner)))\n",
    "            print('Optimal player = ' +  players[0])\n",
    "            print('Agent player = ' +  players[1])\n",
    "            env.render()\n",
    "\n",
    "        if game != 0 and game%1000 == 0:\n",
    "            t_end = time.time()\n",
    "            print('Time for games {}-{}: {:.2f}s <-> {:.2f}m'.format(game_start, game,(t_end - t_start), (t_end - t_start)/60))\n",
    "            t_start = time.time()\n",
    "            game_start = game\n",
    "\n",
    "    return Q_table.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Game 0 ended, winner is player None\n",
      "Optimal player = O\n",
      "Agent player = X\n",
      "|X O O|\n",
      "|O X X|\n",
      "|X X O|\n",
      "\n",
      "-------------------------------------------\n",
      "Game 1 ended, winner is player O\n",
      "Optimal player = X\n",
      "Agent player = O\n",
      "|O X X|\n",
      "|O X X|\n",
      "|O O -|\n",
      "\n",
      "-------------------------------------------\n",
      "Game 2 ended, winner is player O\n",
      "Optimal player = O\n",
      "Agent player = X\n",
      "|O X X|\n",
      "|- O -|\n",
      "|- X O|\n",
      "\n",
      "-------------------------------------------\n",
      "Game 3 ended, winner is player X\n",
      "Optimal player = X\n",
      "Agent player = O\n",
      "|- - X|\n",
      "|- X O|\n",
      "|X - O|\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-227-0e932d724d66>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgenerted_Q_table\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mq_learning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-226-1b24c287aade>\u001b[0m in \u001b[0;36mq_learning\u001b[1;34m(eps)\u001b[0m\n\u001b[0;32m     51\u001b[0m                     \u001b[0mnext_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction_to_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplayer_opt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m                     \u001b[0mnext_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpick_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ_table\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[1;31m# update Q-table using the iterative update rule\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-214-93b1436c9d63>\u001b[0m in \u001b[0;36mpick_action\u001b[1;34m(state, Q_table, epsilon)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;31m# Get index of Q_table corresponding to current state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[0mstate_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_Q_table_idx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ_table\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;31m# Get all the valid actions converted to integers between 0 and 8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-214-93b1436c9d63>\u001b[0m in \u001b[0;36mfind_Q_table_idx\u001b[1;34m(Q_table, grid)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m#print(type(state))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36marray_equal\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36marray_equal\u001b[1;34m(a1, a2, equal_nan)\u001b[0m\n\u001b[0;32m   2451\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2452\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mequal_nan\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2453\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma1\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0ma2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2454\u001b[0m     \u001b[1;31m# Handling NaN values if equal_nan is True\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2455\u001b[0m     \u001b[0ma1nan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma2nan\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_all\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;31m# Parsing keyword arguments is currently fairly slow, so avoid it for now\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mwhere\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mumr_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mumr_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "generted_Q_table = q_learning(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1. -1.]\n",
      "[1. 1.]\n",
      "(5, 10)\n",
      "[[0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]]\n",
      "[[array([[ 1.,  1., -1.],\n",
      "         [-1.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.]]) 0 0 0 0 0 0 0 0 0]\n",
      " [array([[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]]) 0 0 0 0 0 0 0 0 0]\n",
      " [array([[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]]) 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]]\n",
      "[array([[ 1.,  1., -1.],\n",
      "        [-1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.]]) 1 2 3 41 5 61 7 61 9]\n",
      "(5, 10)\n",
      "[(1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "[4, 5, 6, 7, 8]\n",
      "([4, 5, 6, 7, 8], array([5, 61, 7, 61, 9], dtype=object))\n",
      "[1 3]\n",
      "1\n",
      "5\n",
      "[[array([[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]]) 1 2 3 41 5 61 7 61 9]\n",
      " [array([[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]]) 11 12 13 14 15 16 17 18 19]\n",
      " [array([[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]]) 21 22 23 24 25 26 27 28 28]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]]\n",
      "0\n",
      "[[array([[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]]) 1 2 3 41 5 61 7 61 array([[1., 1., 1.],\n",
      "                                                  [1., 1., 1.],\n",
      "                                                  [1., 1., 1.]])]\n",
      " [array([[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]]) 11 12 13 14 15 16 17 18 19]\n",
      " [array([[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]]) 21 22 23 24 25 26 27 28 28]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "grid = np.zeros((3,3))\n",
    "grid_ = grid.copy()\n",
    "grid_[0,0] = 1\n",
    "grid_[1,0] = -1\n",
    "grid_[0,1] = 1\n",
    "grid_[0,2] = -1\n",
    "\n",
    "print(grid_[0,:])\n",
    "print(grid_[0, :-1])\n",
    "\n",
    "q = np.zeros((5, 10), dtype=object)\n",
    "#print(grid)\n",
    "print(q.shape)\n",
    "print(q)\n",
    "q[0, 0] = grid_\n",
    "q[1, 0] = grid\n",
    "q[2, 0] = grid\n",
    "print(q)\n",
    "q[0, 1:] = [1,2,3,41,5,61,7,61,9]\n",
    "q[1, 1:] = [11,12,13,14,15,16,17,18,19]\n",
    "q[2, 1:] = [21,22,23,24,25,26,27,28,28]\n",
    "print(q[0,:])\n",
    "print(q.shape)\n",
    "\n",
    "valid_actions = np.where(q[0, 0]==0)\n",
    "print(list(zip(valid_actions[0], valid_actions[1])))\n",
    "\n",
    "idx_valid_actions = [3*x + y for x, y in zip(valid_actions[0], valid_actions[1])]\n",
    "print(idx_valid_actions)\n",
    "\n",
    "action_scores_tup = (idx_valid_actions, q[0, 1:][idx_valid_actions])\n",
    "print(action_scores_tup)\n",
    "idx_max_action = np.where(action_scores_tup[1] == np.max(action_scores_tup[1]))[0]\n",
    "print(idx_max_action)\n",
    "random_idx = np.random.choice(idx_max_action)\n",
    "print(random_idx)\n",
    "print(action_scores_tup[0][random_idx])\n",
    "\n",
    "\n",
    "def update(q):\n",
    "    q[0,0] = grid\n",
    "\n",
    "update(q)\n",
    "print(q)\n",
    "\n",
    "idx = find_Q_table_idx(q, np.ones((3,3)))\n",
    "print(idx)\n",
    "print(q)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d643e56c8356b332e138988d3326f90faf1438da2687a45045158120a5c71be7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
